import streamlit as st
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from langchain_community.embeddings import HuggingFaceEmbeddings
import os

class MultiDatasetRetriever:
    def __init__(self):
        self.datasets = {}
        self.embeddings_model = None
        
    def load_dataset(self, name, text_csv_path, full_csv_path, embeddings_npy_path):
        try:
            text_df = pd.read_csv(text_csv_path)
            full_df = pd.read_csv(full_csv_path)
            
            # ì„ë² ë”© íŒŒì¼ ë¡œë“œ ì‹œë„ (ì‹¤íŒ¨í•´ë„ í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´)
            try:
                embeddings = np.load(embeddings_npy_path)
                use_embeddings = True
            except:
                embeddings = None
                use_embeddings = False
                st.warning(f"ì„ë² ë”© íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ì–´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            self.datasets[name] = {
                'text_df': text_df,
                'full_df': full_df,
                'embeddings': embeddings,
                'text_column': 'text',
                'use_embeddings': use_embeddings
            }
            
            return True
            
        except Exception as e:
            st.error(f"ë°ì´í„°ì…‹ '{name}' ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def search_similar_docs(self, query, dataset_name, top_k=5):
        if dataset_name not in self.datasets:
            return []
            
        dataset = self.datasets[dataset_name]
        
        if dataset['use_embeddings'] and dataset['embeddings'] is not None:
            # ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰
            try:
                if self.embeddings_model is None:
                    # ì°¨ì›ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¼ê´€ì„± ë³´ì¥
                    self.embeddings_model = HuggingFaceEmbeddings(
                        model_name="BAAI/bge-m3",
                        model_kwargs={'device': 'cpu'},
                        encode_kwargs={'normalize_embeddings': True}
                    )
                
                query_embedding = self.embeddings_model.embed_query(query)
                query_embedding = np.array(query_embedding).reshape(1, -1)
                
                similarities = cosine_similarity(query_embedding, dataset['embeddings'])[0]
                top_indices = np.argsort(similarities)[::-1][:top_k]
                
                results = []
                for idx in top_indices:
                    similarity_score = similarities[idx]
                    text_content = dataset['text_df'].iloc[idx][dataset['text_column']]
                    full_data = dataset['full_df'].iloc[idx].to_dict()
                    
                    results.append({
                        'content': text_content,
                        'similarity': similarity_score,
                        'full_data': full_data,
                        'index': idx
                    })
                    
                return results
                
            except Exception as e:
                st.warning(f"ì„ë² ë”© ê²€ìƒ‰ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´: {e}")
                dataset['use_embeddings'] = False
        
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (ì„ë² ë”© ì‹¤íŒ¨ ì‹œ)
        if not dataset['use_embeddings']:
            query_lower = query.lower()
            results = []
            
            for idx, row in dataset['text_df'].iterrows():
                text_content = row[dataset['text_column']]
                text_lower = text_content.lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                score = 0
                for word in query_lower.split():
                    if word in text_lower:
                        score += 1
                
                if score > 0:
                    results.append({
                        'content': text_content,
                        'similarity': score / len(query_lower.split()),  # ì •ê·œí™”ëœ ì ìˆ˜
                        'full_data': dataset['full_df'].iloc[idx].to_dict(),
                        'index': idx
                    })
            
            # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda x: x['similarity'], reverse=True)
            return results[:top_k]
        
        return []
    
    def get_available_datasets(self):
        return list(self.datasets.keys())

@st.cache_resource(show_spinner=False)
def load_datasets():
    with st.spinner("ğŸ”„ ë°ì´í„°ì…‹ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        retriever = MultiDatasetRetriever()
        
        datasets_config = {
            "ì‹¤ì œ ë°ì´í„°": {
                "text_csv": "./data/text_ex.csv",
                "full_csv": "./data/full_data_ex.csv", 
                "embeddings": "./data/embeddings_ex.npy"
            }
        }
        
        loaded_datasets = []
        for name, config in datasets_config.items():
            if retriever.load_dataset(name, config["text_csv"], config["full_csv"], config["embeddings"]):
                loaded_datasets.append(name)
        
        return retriever, loaded_datasets 