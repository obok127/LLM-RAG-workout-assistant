import pandas as pd
import numpy as np
import streamlit as st
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# For embed_file utility
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore

class DataLoader:
    """ë°ì´í„° ë¡œë”© ë° ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: str = "./data"):
        self.data_dir = Path(data_dir)
        self.datasets = {}
        self.embeddings = {}
    
    def load_csv_data(self, file_path: str) -> Optional[pd.DataFrame]:
        """CSV íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
        try:
            full_path = self.data_dir / file_path
            if full_path.exists():
                df = pd.read_csv(full_path)
                st.success(f"âœ… {file_path} ë¡œë“œ ì™„ë£Œ ({len(df)}í–‰)")
                return df
            else:
                st.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
                return None
        except Exception as e:
            st.error(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_embeddings(self, file_path: str) -> Optional[np.ndarray]:
        """ì„ë² ë”© íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
        try:
            full_path = self.data_dir / file_path
            if full_path.exists():
                embeddings = np.load(full_path)
                st.success(f"âœ… {file_path} ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {embeddings.shape})")
                return embeddings
            else:
                st.warning(f"âš ï¸ ì„ë² ë”© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
                return None
        except Exception as e:
            st.warning(f"âš ï¸ ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_dataset(self, name: str, text_csv: str, full_csv: str, embeddings_npy: str) -> bool:
        """ì „ì²´ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
        try:
            # í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
            text_df = self.load_csv_data(text_csv)
            if text_df is None:
                return False
            
            # ì „ì²´ ë°ì´í„° ë¡œë“œ
            full_df = self.load_csv_data(full_csv)
            if full_df is None:
                return False
            
            # ì„ë² ë”© ë¡œë“œ (ì„ íƒì )
            embeddings = self.load_embeddings(embeddings_npy)
            
            # ë°ì´í„°ì…‹ ì €ì¥
            self.datasets[name] = {
                'text_df': text_df,
                'full_df': full_df,
                'embeddings': embeddings,
                'text_column': 'text',
                'use_embeddings': embeddings is not None
            }
            
            return True
            
        except Exception as e:
            st.error(f"âŒ ë°ì´í„°ì…‹ '{name}' ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_dataset_info(self, name: str) -> Optional[Dict]:
        """ë°ì´í„°ì…‹ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        if name not in self.datasets:
            return None
        
        dataset = self.datasets[name]
        text_df = dataset['text_df']
        full_df = dataset['full_df']
        embeddings = dataset['embeddings']
        
        info = {
            'name': name,
            'text_rows': len(text_df),
            'full_rows': len(full_df),
            'text_columns': list(text_df.columns),
            'full_columns': list(full_df.columns),
            'has_embeddings': embeddings is not None,
            'embedding_shape': embeddings.shape if embeddings is not None else None
        }
        
        return info
    
    def get_available_datasets(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        return list(self.datasets.keys())
    
    def validate_data_consistency(self, name: str) -> bool:
        """ë°ì´í„° ì¼ê´€ì„±ì„ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜"""
        if name not in self.datasets:
            return False
        
        dataset = self.datasets[name]
        text_df = dataset['text_df']
        full_df = dataset['full_df']
        
        # í–‰ ìˆ˜ ì¼ì¹˜ í™•ì¸
        if len(text_df) != len(full_df):
            st.error(f"âŒ ë°ì´í„° ë¶ˆì¼ì¹˜: text_df({len(text_df)}í–‰) != full_df({len(full_df)}í–‰)")
            return False
        
        # ì„ë² ë”© ì°¨ì› í™•ì¸
        if dataset['embeddings'] is not None:
            if len(dataset['embeddings']) != len(text_df):
                st.error(f"âŒ ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: embeddings({len(dataset['embeddings'])}ê°œ) != text_df({len(text_df)}í–‰)")
                return False
        
        st.success(f"âœ… ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì™„ë£Œ: {name}")
        return True
    
    def get_sample_data(self, name: str, n_samples: int = 5) -> Optional[Dict]:
        """ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        if name not in self.datasets:
            return None
        
        dataset = self.datasets[name]
        text_df = dataset['text_df']
        full_df = dataset['full_df']
        
        # ëœë¤ ìƒ˜í”Œ ì„ íƒ
        sample_indices = np.random.choice(len(text_df), min(n_samples, len(text_df)), replace=False)
        
        samples = []
        for idx in sample_indices:
            sample = {
                'text': text_df.iloc[idx][dataset['text_column']],
                'full_data': full_df.iloc[idx].to_dict(),
                'index': idx
            }
            samples.append(sample)
        
        return {
            'dataset_name': name,
            'samples': samples,
            'total_rows': len(text_df)
        }
    
    def search_text_data(self, query: str, name: str, top_k: int = 5) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
        if name not in self.datasets:
            return []
        
        dataset = self.datasets[name]
        text_df = dataset['text_df']
        full_df = dataset['full_df']
        
        query_lower = query.lower()
        results = []
        
        for idx, row in text_df.iterrows():
            text_content = row[dataset['text_column']]
            text_lower = text_content.lower()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            score = 0
            for word in query_lower.split():
                if word in text_lower:
                    score += 1
            
            if score > 0:
                results.append({
                    'content': text_content,
                    'similarity': score / len(query_lower.split()),  # ì •ê·œí™”ëœ ì ìˆ˜
                    'full_data': full_df.iloc[idx].to_dict(),
                    'index': idx
                })
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]


def embed_file(file):
    """ì—…ë¡œë“œ íŒŒì¼ì„ ë¶„í• /ì„ë² ë”©í•´ FAISS ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    with st.spinner(f"ğŸ“„ {getattr(file, 'name', 'uploaded')} ì²˜ë¦¬ ì¤‘..."):
        # ì €ì¥ ë° ê²½ë¡œ ì¤€ë¹„
        save_dir = Path("./cache/files")
        save_dir.mkdir(parents=True, exist_ok=True)
        file_name = getattr(file, 'name', 'uploaded_file')
        file_path = save_dir / file_name
        # íŒŒì¼ ì €ì¥
        content = file.read()
        with open(file_path, "wb") as f_out:
            f_out.write(content)

        # ë¬¸ì„œ ë¡œë“œ/ë¶„í• 
        cache_dir = LocalFileStore(f"./.cache/embeddings/{file_name}")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", r"(?<=\.)", " ", ""],
            length_function=len,
        )
        loader = UnstructuredFileLoader(str(file_path))
        docs = loader.load_and_split(text_splitter=text_splitter)

        # ì„ë² ë”©/ë²¡í„°ìŠ¤í† ì–´
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
        vectorstore = FAISS.from_documents(docs, cached_embeddings)
        return vectorstore.as_retriever()

@st.cache_resource(show_spinner=False)
def create_data_loader() -> DataLoader:
    """ë°ì´í„° ë¡œë” ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ìºì‹œë¨)"""
    return DataLoader()

@st.cache_data(show_spinner=False)
def load_all_datasets() -> Dict[str, bool]:
    """ëª¨ë“  ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (ìºì‹œë¨)"""
    with st.spinner("ğŸ”„ ë°ì´í„°ì…‹ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        loader = create_data_loader()
        
        datasets_config = {
            "ì‹¤ì œ ë°ì´í„°": {
                "text_csv": "text_ex.csv",
                "full_csv": "full_data_ex.csv", 
                "embeddings": "embeddings_ex.npy"
            }
        }
        
        results = {}
        for name, config in datasets_config.items():
            success = loader.load_dataset(
                name, 
                config["text_csv"], 
                config["full_csv"], 
                config["embeddings"]
            )
            results[name] = success
            
            if success:
                loader.validate_data_consistency(name)
        
        return results 