import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain_community.document_loaders import UnstructuredFileLoader, TextLoader, DirectoryLoader
from langchain.embeddings import CacheBackedEmbeddings
from langchain.schema.runnable import RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import EnsembleRetriever
import os
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import time
import plotly.graph_objects as go
import plotly.express as px

# ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ import
from ui.styles import CUSTOM_CSS
from core.prompt_builder import AdvancedPromptBuilder, extract_body_part_and_goal, _analyze_question_type, generate_nori_prompt
from core.retriever import MultiDatasetRetriever, load_datasets
from utils.helpers import print_history, add_history, format_docs
from utils.data_loader import embed_file

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI ìš´ë™ìì„¸ ì–´ì‹œìŠ¤í„´íŠ¸",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS ì ìš©
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

# í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ’ª AI ìš´ë™ í”„ë¡œê·¸ë¨ ì–´ì‹œìŠ¤í„´íŠ¸</h1>
    <p>ìš´ë™ í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ìœ„í•œ ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ</p>
</div>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "search_history" not in st.session_state:
    st.session_state["search_history"] = []
if "system_stats" not in st.session_state:
    st.session_state["system_stats"] = {
        "total_searches": 0,
        "avg_response_time": 0.0,
        "datasets_used": set()
    }





# ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ì—ì„œ í´ë˜ìŠ¤ë“¤ì„ importí•˜ì—¬ ì‚¬ìš©

# ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ì—ì„œ í´ë˜ìŠ¤ë“¤ì„ importí•˜ì—¬ ì‚¬ìš©

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
prompt_builder = AdvancedPromptBuilder()

# ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ì—ì„œ í•¨ìˆ˜ë“¤ì„ importí•˜ì—¬ ì‚¬ìš©

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.markdown("### ğŸ›ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    
    # ë¬¸ì„œ ì†ŒìŠ¤ ì„ íƒ
    doc_source = st.radio(
        "ğŸ“š ë¬¸ì„œ ì†ŒìŠ¤",
        ["ğŸ—‚ï¸ ë‹¤ì¤‘ ë°ì´í„°ì…‹", "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ"],
        help="ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”"
    )
    
    if doc_source == "ğŸ—‚ï¸ ë‹¤ì¤‘ ë°ì´í„°ì…‹":
        st.markdown('<div class="sidebar-card">', unsafe_allow_html=True)
        st.markdown("#### ğŸ“Š ë°ì´í„°ì…‹ í˜„í™©")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        multi_retriever, available_datasets = load_datasets()
        
        if available_datasets:
            st.markdown(f'<div class="status-success">âœ… {len(available_datasets)}ê°œ ë°ì´í„°ì…‹ í™œì„±í™”</div>', unsafe_allow_html=True)
            
            # ë°ì´í„°ì…‹ ì„ íƒ
            selected_datasets = st.multiselect(
                "ê²€ìƒ‰ ëŒ€ìƒ ì„ íƒ",
                available_datasets,
                default=available_datasets[:3],  # ì²˜ìŒ 3ê°œë§Œ ê¸°ë³¸ ì„ íƒ
                help="ê²€ìƒ‰í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”"
            )
            
            # ê³ ê¸‰ ì„¤ì •
            with st.expander("âš™ï¸ ê³ ê¸‰ ì„¤ì •"):
                top_k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜", 1, 20, 5)
                confidence_threshold = st.slider("ì‹ ë¢°ë„ ì„ê³„ê°’", 0.1, 1.0, 0.7, 0.1)
                
        else:
            st.markdown('<div class="status-error">âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨</div>', unsafe_allow_html=True)
            selected_datasets = []
            
        st.markdown('</div>', unsafe_allow_html=True)
            
    else:  # íŒŒì¼ ì—…ë¡œë“œ
        st.markdown('<div class="sidebar-card">', unsafe_allow_html=True)
        st.markdown("#### ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
        
        files = st.file_uploader(
            "íŒŒì¼ ì„ íƒ", 
            type=["pdf", "txt", "docx"], 
            accept_multiple_files=True,
            help="PDF, TXT, DOCX íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
        )
        
        retriever = None
        if files:
            st.markdown(f'<div class="status-success">âœ… {len(files)}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨</div>', unsafe_allow_html=True)
            retrievers = [embed_file(f) for f in files]
            retriever = EnsembleRetriever(retrievers=retrievers)
            
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ì‹œìŠ¤í…œ í†µê³„
    st.markdown("### ğŸ“ˆ ì‹œìŠ¤í…œ í†µê³„")
    stats = st.session_state["system_stats"]
    
    st.markdown(f"""
    <div class="metric-card">
        <h3>{stats['total_searches']}</h3>
        <p>ì´ ê²€ìƒ‰ íšŸìˆ˜</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown(f"""
    <div class="metric-card">
        <h3>{stats['avg_response_time']:.2f}s</h3>
        <p>í‰ê·  ì‘ë‹µ ì‹œê°„</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown(f"""
    <div class="metric-card">
        <h3>{len(stats['datasets_used'])}</h3>
        <p>ì‚¬ìš©ëœ ë°ì´í„°ì…‹</p>
    </div>
    """, unsafe_allow_html=True)

# í”„ë¡¬í”„íŠ¸ ë¹Œë” ì´ˆê¸°í™”
prompt_builder = AdvancedPromptBuilder()

# ë©”ì¸ ì±„íŒ… ì˜ì—­
st.markdown("### ğŸš€ AI ìš´ë™ í”„ë¡œê·¸ë¨ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
if st.session_state["messages"]:
    print_history()
else:

    
    # ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼ë“¤
    st.markdown("### ğŸ’¡ ë¹ ë¥¸ ì§ˆë¬¸í•˜ê¸°")
    
    # ë²„íŠ¼ 1
    if st.button("ğŸ’ª ìƒì²´ ìš´ë™ì„ ì•Œë ¤ì£¼ì„¸ìš”", key="btn1", use_container_width=True):
        st.session_state["quick_question"] = "ìƒì²´ ìš´ë™ì„ ì•Œë ¤ì£¼ì„¸ìš”"
        st.rerun()
    
    # ë²„íŠ¼ 2
    if st.button("ğŸ¦µ í•˜ì²´ ê°•í™” ìš´ë™ í”„ë¡œê·¸ë¨ì„ ì°¾ì•„ì£¼ì„¸ìš”", key="btn2", use_container_width=True):
        st.session_state["quick_question"] = "í•˜ì²´ ê°•í™” ìš´ë™ í”„ë¡œê·¸ë¨ì„ ì°¾ì•„ì£¼ì„¸ìš”"
        st.rerun()
    
    # ë²„íŠ¼ 3
    if st.button("ğŸ‹ï¸ ë³µê·¼ ìš´ë™ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”", key="btn3", use_container_width=True):
        st.session_state["quick_question"] = "ë³µê·¼ ìš´ë™ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”"
        st.rerun()
    
    # ë²„íŠ¼ 4
    if st.button("âœ‹ ì†ëª©ì´ ì•„í”Œ ë•ŒëŠ” ì–´ë–¤ ìš´ë™ì„ í•  ìˆ˜ ìˆë‚˜ìš”?", key="btn4", use_container_width=True):
        st.session_state["quick_question"] = "ì†ëª©ì´ ì•„í”Œ ë•ŒëŠ” ì–´ë–¤ ìš´ë™ì„ í•  ìˆ˜ ìˆë‚˜ìš”?"
        st.rerun()

# ì‚¬ìš©ì ì…ë ¥ - ì‚¬ì´ë“œë°” ë°–ìœ¼ë¡œ ì´ë™
user_input = st.chat_input("ğŸ’­ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

# ë¹ ë¥¸ ì§ˆë¬¸ ë²„íŠ¼ ì²˜ë¦¬
if "quick_question" in st.session_state:
    user_input = st.session_state["quick_question"]
    del st.session_state["quick_question"]  # í•œ ë²ˆ ì‚¬ìš© í›„ ì‚­ì œ

if user_input:
    # ì‘ë‹µ ì‹œê°„ ì¸¡ì • ì‹œì‘
    start_time = time.time()
    
    add_history("user", user_input)
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.markdown(f"""
    <div style="background: #2c3e50; 
               color: white; padding: 1rem; border-radius: 10px; 
               margin: 0.5rem 0; margin-left: 20%;">
        <strong>ğŸ™‹â€â™‚ï¸ ì‚¬ìš©ì:</strong> {user_input}
    </div>
    """, unsafe_allow_html=True)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.spinner("ğŸ¤– AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        # Ollama ì—°ê²° ì„¤ì •
        ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        ollama = Ollama(model="eeve-korean-10-8b", base_url=ollama_base_url)
        
        if doc_source == "ğŸ—‚ï¸ ë‹¤ì¤‘ ë°ì´í„°ì…‹" and selected_datasets:
            # ë‹¤ì¤‘ ë°ì´í„°ì…‹ ê²€ìƒ‰
            all_results = []
            for dataset_name in selected_datasets:
                results = multi_retriever.search_similar_docs(user_input, dataset_name, top_k)
                for result in results:
                    result['dataset'] = dataset_name
                    all_results.append(result)
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            all_results.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = all_results[:top_k]
            
            if top_results:
                # ì»¨í…ìŠ¤íŠ¸ ìƒì„± - ìš´ë™ ë°ì´í„°ë¥¼ ë” ëª…í™•í•˜ê²Œ êµ¬ì¡°í™”
                context_parts = []
                for i, result in enumerate(top_results, 1):
                    # full_dataì—ì„œ ìš´ë™ ì •ë³´ ì¶”ì¶œ
                    full_data = result['full_data']
                    exercise_info = f"""
=== ìš´ë™ ë°ì´í„° {i} (ìœ ì‚¬ë„: {result['similarity']:.3f}) ===
ìš´ë™ëª…: {full_data.get('ìš´ë™ëª…', 'N/A')}
ëª©ì : {full_data.get('ëª©ì ', 'N/A')}
ì„¤ëª…: {full_data.get('ì„¤ëª…', 'N/A')}
ì²´ë ¥ ìš”ì†Œ: {full_data.get('ì²´ë ¥ ìš”ì†Œ', 'N/A')}
ìš´ë™ ë¶€ìœ„: {full_data.get('ìš´ë™ ë¶€ìœ„', 'N/A')}
ë„êµ¬: {full_data.get('ë„êµ¬', 'N/A')}
ì œì‘ì—°ë„: {full_data.get('ì œì‘ì—°ë„', 'N/A')}
ì˜ìƒ ë§í¬: {full_data.get('ì˜ìƒ ë§í¬', 'N/A')}
"""
                    context_parts.append(exercise_info)
                
                context = "\n\n".join(context_parts)
                
                # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
                avg_confidence = sum(r['similarity'] for r in top_results) / len(top_results)
                
                # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ì¼ë°˜ ëŒ€í™” + RAG ì •ë³´ ì œê³µ
                if avg_confidence < 0.3:
                    hybrid_prompt = f"""ë‹¹ì‹ ì€ ìš´ë™ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹¤ìŒì€ ê´€ë ¨ë  ìˆ˜ ìˆëŠ” ì°¸ê³  ì •ë³´ì…ë‹ˆë‹¤ (ì‹ ë¢°ë„ê°€ ë‚®ì•„ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©):
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

ë‹µë³€ (ìš´ë™ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° 4ê°œ ìš´ë™ í”„ë¡œê·¸ë¨ í˜•íƒœë¡œ, ì¼ë°˜ ì§ˆë¬¸ì¸ ê²½ìš° ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”):"""
                    answer = ollama.invoke(hybrid_prompt)
                else:
                    # í”„ë¡¬í”„íŠ¸ ìƒì„±
                    rag_prompt = prompt_builder.build_rag_prompt(
                        context=context,
                        question=user_input,
                        data_types=selected_datasets,
                        confidence_threshold=confidence_threshold
                    )
                    
                    # ë‹µë³€ ìƒì„±
                    answer = ollama.invoke(rag_prompt)
                
                # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
                with st.expander("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´"):
                    for i, result in enumerate(top_results):
                        st.markdown(f"""
                        <div class="search-result-card">
                            <strong>{i+1}. [{result['dataset']}] (ìœ ì‚¬ë„: {result['similarity']:.3f})</strong><br>
                            <small>ë‚´ìš©: {result['content'][:200]}...</small>
                        </div>
                        """, unsafe_allow_html=True)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                response_time = time.time() - start_time
                st.session_state["system_stats"]["total_searches"] += 1
                st.session_state["system_stats"]["datasets_used"].update(selected_datasets)
                
                # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
                current_avg = st.session_state["system_stats"]["avg_response_time"]
                total_searches = st.session_state["system_stats"]["total_searches"]
                new_avg = (current_avg * (total_searches - 1) + response_time) / total_searches
                st.session_state["system_stats"]["avg_response_time"] = new_avg
                
            else:
                # RAG ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì „í™˜
                general_prompt = f"""ë‹¹ì‹ ì€ ìš´ë™ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ìš´ë™ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°:
- 4ê°œ ìš´ë™ í”„ë¡œê·¸ë¨ í˜•íƒœë¡œ ë‹µë³€
- ëª©ì ì— ë§ëŠ” êµ¬ì²´ì ì¸ ìš´ë™ ì¶”ì²œ
- ê° ìš´ë™ë³„ ìƒì„¸ ì„¤ëª… ì œê³µ

ì¼ë°˜ ì§ˆë¬¸ì¸ ê²½ìš°:
- ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”

ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

ë‹µë³€:"""
                
                answer = ollama.invoke(general_prompt)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                response_time = time.time() - start_time
                st.session_state["system_stats"]["total_searches"] += 1
                
                current_avg = st.session_state["system_stats"]["avg_response_time"]
                total_searches = st.session_state["system_stats"]["total_searches"]
                new_avg = (current_avg * (total_searches - 1) + response_time) / total_searches
                st.session_state["system_stats"]["avg_response_time"] = new_avg
                
        elif doc_source == "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ" and retriever:
            # íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹
            retrieved_docs = retriever.invoke(user_input)
            context = format_docs(retrieved_docs)
            
            rag_prompt = prompt_builder.build_rag_prompt(
                context=context,
                question=user_input,
                data_types=["ì—…ë¡œë“œëœ íŒŒì¼"]
            )
            
            answer = ollama.invoke(rag_prompt)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            response_time = time.time() - start_time
            st.session_state["system_stats"]["total_searches"] += 1
            
            current_avg = st.session_state["system_stats"]["avg_response_time"]
            total_searches = st.session_state["system_stats"]["total_searches"]
            new_avg = (current_avg * (total_searches - 1) + response_time) / total_searches
            st.session_state["system_stats"]["avg_response_time"] = new_avg
            
        else:
            # ì¼ë°˜ ì§ˆë¬¸ (ë°ì´í„° ì—†ì´)
            if doc_source == "ğŸ—‚ï¸ ë‹¤ì¤‘ ë°ì´í„°ì…‹":
                answer = "âŒ ë°ì´í„°ì…‹ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
            else:
                answer = "âŒ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        # AI ì‘ë‹µ í‘œì‹œ
        st.markdown(f"""
        <div style="background: #f8f9fa; 
                   padding: 1.5rem; border-radius: 15px; 
                   margin: 0.5rem 0; margin-right: 20%;
                   border-left: 5px solid #3498db;
                   box-shadow: 0 4px 15px rgba(0,0,0,0.1);
                   color: #2c3e50;">
            <strong style="color: #3498db; font-size: 1.1em;">ğŸ¤– AI ì–´ì‹œìŠ¤í„´íŠ¸:</strong><br><br>
            <div style="line-height: 1.6; color: #2c3e50;">{answer}</div>
        </div>
        """, unsafe_allow_html=True)
        
        add_history("assistant", answer)
        
        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        st.rerun() 